# Set seed for reproducibility
set.seed(42)
# Define the parameters for the network
sizes <- c(50, 50, 50, 50)  # Sizes of each community, totaling 200 nodes
p_in <- 0.15  # Reduced probability of edges within communities to decrease modularity
p_out <- 0.1  # Increase probability of edges between communities to reduce modularity
# Create the stochastic block model with `sample_sbm`
network <- sample_sbm(
n = sum(sizes),                # Total number of nodes
pref.matrix = matrix(
c(p_in, p_out, p_out, p_out,
p_out, p_in, p_out, p_out,
p_out, p_out, p_in, p_out,
p_out, p_out, p_out, p_in),
nrow = 4, ncol = 4
),
block.sizes = sizes,           # Block sizes for the communities
directed = FALSE, loops = FALSE
)
# Add community information to nodes for color coding
network <- set_vertex_attr(network, "community", value = membership(cluster_louvain(network)))
# Plot the network with ggraph
ggraph(network, layout = "fr") +             # Fruchterman-Reingold layout
geom_edge_link(color = "black", alpha = 0.1) +   # Translucent edges
geom_node_point(aes(color = as.factor(community)), size = 5) +  # Opaque nodes
theme_void() +                            # Removes all grid lines, axes, and background
theme(
legend.position = "none",               # Removes the legend
plot.background = element_rect(fill = "#EFEBE6", color = NA)  # Original background color
)
# Load necessary libraries
library(igraph)
library(ggraph)
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Define the parameters for the network
sizes <- c(50, 50, 50, 50)  # Sizes of each community, totaling 200 nodes
p_in <- 0.2  # Moderate probability of edges within communities
p_out <- 0.07  # Moderate probability of edges between communities
# Create the stochastic block model with `sample_sbm`
network <- sample_sbm(
n = sum(sizes),                # Total number of nodes
pref.matrix = matrix(
c(p_in, p_out, p_out, p_out,
p_out, p_in, p_out, p_out,
p_out, p_out, p_in, p_out,
p_out, p_out, p_out, p_in),
nrow = 4, ncol = 4
),
block.sizes = sizes,           # Block sizes for the communities
directed = FALSE, loops = FALSE
)
# Add community information to nodes for color coding
network <- set_vertex_attr(network, "community", value = membership(cluster_louvain(network)))
# Plot the network with ggraph
ggraph(network, layout = "fr") +             # Fruchterman-Reingold layout
geom_edge_link(color = "black", alpha = 0.1) +   # Translucent edges
geom_node_point(aes(color = as.factor(community)), size = 5) +  # Opaque nodes
theme_void() +                            # Removes all grid lines, axes, and background
theme(
legend.position = "none",               # Removes the legend
plot.background = element_rect(fill = "#EFEBE6", color = NA)  # Original background color
)
# Load necessary libraries
library(igraph)
library(ggraph)
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Define the parameters for the network
sizes <- c(25, 25, 25, 25)  # Sizes of each community, totaling 100 nodes
p_in <- 0.2  # Moderate probability of edges within communities
p_out <- 0.07  # Moderate probability of edges between communities
# Create the stochastic block model with `sample_sbm`
network <- sample_sbm(
n = sum(sizes),                # Total number of nodes
pref.matrix = matrix(
c(p_in, p_out, p_out, p_out,
p_out, p_in, p_out, p_out,
p_out, p_out, p_in, p_out,
p_out, p_out, p_out, p_in),
nrow = 4, ncol = 4
),
block.sizes = sizes,           # Block sizes for the communities
directed = FALSE, loops = FALSE
)
# Randomly assign thickness to edges for visual variation
E(network)$weight <- sample(c(0.5, 1.5, 2.5), ecount(network), replace = TRUE)
# Plot the network with ggraph
ggraph(network, layout = "fr") +             # Fruchterman-Reingold layout
geom_edge_link(aes(width = weight), color = "black", alpha = 0.1) +   # Translucent edges with varying thickness
geom_node_point(color = "darkred", size = 5) +  # Opaque nodes without cluster colors
scale_edge_width(range = c(0.5, 2.5)) +         # Scale for edge thickness
theme_void() +                            # Removes all grid lines, axes, and background
theme(
legend.position = "none",               # Removes the legend
plot.background = element_rect(fill = "#EFEBE6", color = NA)  # Background color matching slide
)
# Load necessary libraries
library(igraph)
library(ggraph)
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Define the parameters for the network
sizes <- c(25, 25, 25, 25)  # Sizes of each community, totaling 100 nodes
p_in <- 0.2  # Moderate probability of edges within communities
p_out <- 0.07  # Moderate probability of edges between communities
# Create the stochastic block model with `sample_sbm`
network <- sample_sbm(
n = sum(sizes),                # Total number of nodes
pref.matrix = matrix(
c(p_in, p_out, p_out, p_out,
p_out, p_in, p_out, p_out,
p_out, p_out, p_in, p_out,
p_out, p_out, p_out, p_in),
nrow = 4, ncol = 4
),
block.sizes = sizes,           # Block sizes for the communities
directed = FALSE, loops = FALSE
)
# Randomly assign thickness to edges for visual variation
E(network)$weight <- sample(c(0.5, 1.5, 2.5), ecount(network), replace = TRUE)
# Plot the network with ggraph
ggraph(network, layout = "fr") +             # Fruchterman-Reingold layout
geom_edge_link(aes(width = weight), color = "black", alpha = 0.1) +   # Translucent edges with varying thickness
geom_node_point(color = "lightblue", size = 5) +  # Opaque nodes without cluster colors
scale_edge_width(range = c(0.5, 2.5)) +         # Scale for edge thickness
theme_void() +                            # Removes all grid lines, axes, and background
theme(
legend.position = "none",               # Removes the legend
plot.background = element_rect(fill = "#EFEBE6", color = NA)  # Background color matching slide
)
# Load necessary libraries
library(igraph)
library(ggraph)
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Define the parameters for the network
sizes <- c(25, 25, 25, 25)  # Sizes of each community, totaling 100 nodes
p_in <- 0.2  # Moderate probability of edges within communities
p_out <- 0.07  # Moderate probability of edges between communities
# Create the stochastic block model with `sample_sbm`
network <- sample_sbm(
n = sum(sizes),                # Total number of nodes
pref.matrix = matrix(
c(p_in, p_out, p_out, p_out,
p_out, p_in, p_out, p_out,
p_out, p_out, p_in, p_out,
p_out, p_out, p_out, p_in),
nrow = 4, ncol = 4
),
block.sizes = sizes,           # Block sizes for the communities
directed = FALSE, loops = FALSE
)
# Randomly assign thickness to edges for visual variation
E(network)$weight <- sample(c(0.5, 1.5, 2.5), ecount(network), replace = TRUE)
# Plot the network with ggraph
ggraph(network, layout = "fr") +             # Fruchterman-Reingold layout
geom_edge_link(aes(width = weight), color = "black", alpha = 0.1) +   # Translucent edges with varying thickness
geom_node_point(color = "darkblue", size = 5) +  # Opaque nodes without cluster colors
scale_edge_width(range = c(0.5, 2.5)) +         # Scale for edge thickness
theme_void() +                            # Removes all grid lines, axes, and background
theme(
legend.position = "none",               # Removes the legend
plot.background = element_rect(fill = "#EFEBE6", color = NA)  # Background color matching slide
)
# Load necessary libraries
library(igraph)
library(ggraph)
library(ggplot2)
# Set seed for reproducibility
set.seed(42)
# Define the parameters for the network
sizes <- c(25, 25, 25, 25)  # Sizes of each community, totaling 100 nodes
p_in <- 0.2  # Moderate probability of edges within communities
p_out <- 0.07  # Moderate probability of edges between communities
# Create the stochastic block model with `sample_sbm`
network <- sample_sbm(
n = sum(sizes),                # Total number of nodes
pref.matrix = matrix(
c(p_in, p_out, p_out, p_out,
p_out, p_in, p_out, p_out,
p_out, p_out, p_in, p_out,
p_out, p_out, p_out, p_in),
nrow = 4, ncol = 4
),
block.sizes = sizes,           # Block sizes for the communities
directed = FALSE, loops = FALSE
)
# Randomly assign thickness to edges for visual variation
E(network)$weight <- sample(c(0.5, 1.5, 2.5), ecount(network), replace = TRUE)
# Plot the network with ggraph
ggraph(network, layout = "fr") +             # Fruchterman-Reingold layout
geom_edge_link(aes(width = weight), color = "black", alpha = 0.1) +   # Translucent edges with varying thickness
geom_node_point(color = "firebrick", size = 5) +  # Opaque nodes without cluster colors
scale_edge_width(range = c(0.5, 2.5)) +         # Scale for edge thickness
theme_void() +                            # Removes all grid lines, axes, and background
theme(
legend.position = "none",               # Removes the legend
plot.background = element_rect(fill = "#EFEBE6", color = NA)  # Background color matching slide
)
gplot(matrix(c(1,1,1,1,1,1,1)))
library(sna)
gplot(matrix(c(1,1,1,1,1,1,1)))
library(DiagrammeR)
# Define an adjacency matrix
adj_matrix <- matrix(c(
0, 1, 1, 0, 0,
1, 0, 1, 1, 0,
1, 1, 0, 1, 1,
0, 1, 1, 0, 1,
0, 0, 1, 1, 0
), nrow = 5, byrow = TRUE)
# Convert the matrix to an edge list
edge_list <- which(adj_matrix == 1, arr.ind = TRUE)
# Ensure edges are only counted once for an undirected graph
edge_list <- edge_list[edge_list[,1] < edge_list[,2], ]
# Create a graph object
graph <- create_graph()
# Add nodes
graph <- graph %>%
add_n_nodes(n = nrow(adj_matrix), label = as.character(1:nrow(adj_matrix)))
# Add edges from the edge list
for (i in 1:nrow(edge_list)) {
graph <- graph %>%
add_edge(from = edge_list[i, 1], to = edge_list[i, 2])
}
# Render the graph
render_graph(graph)
library(DiagrammeR)
set.seed(123) # For reproducibility
# Generate a random adjacency matrix (sparse)
num_nodes <- 1000
prob <- 0.002  # Probability of an edge (adjust for density)
adj_matrix <- matrix(rbinom(num_nodes^2, 1, prob), nrow = num_nodes)
# Ensure symmetry for an undirected graph & remove self-loops
adj_matrix[lower.tri(adj_matrix)] <- adj_matrix[upper.tri(adj_matrix)]
diag(adj_matrix) <- 0
# Convert matrix to edge list
edge_list <- which(adj_matrix == 1, arr.ind = TRUE)
# Ensure edges are only counted once (undirected graph)
edge_list <- edge_list[edge_list[,1] < edge_list[,2], ]
# Create graph object
graph <- create_graph()
# Add nodes (labels as node numbers)
graph <- graph %>%
add_n_nodes(n = num_nodes, label = as.character(1:num_nodes))
# Add edges from the edge list
for (i in 1:nrow(edge_list)) {
graph <- graph %>%
add_edge(from = edge_list[i, 1], to = edge_list[i, 2])
}
# Render the graph (for large graphs, this might be slow)
render_graph(graph)
library(DiagrammeR)
set.seed(123) # For reproducibility
# Generate a random adjacency matrix (sparse)
num_nodes <- 1000
prob <- 0.00002  # Probability of an edge (adjust for density)
adj_matrix <- matrix(rbinom(num_nodes^2, 1, prob), nrow = num_nodes)
# Ensure symmetry for an undirected graph & remove self-loops
adj_matrix[lower.tri(adj_matrix)] <- adj_matrix[upper.tri(adj_matrix)]
diag(adj_matrix) <- 0
# Convert matrix to edge list
edge_list <- which(adj_matrix == 1, arr.ind = TRUE)
# Ensure edges are only counted once (undirected graph)
edge_list <- edge_list[edge_list[,1] < edge_list[,2], ]
# Create graph object
graph <- create_graph()
# Add nodes (labels as node numbers)
graph <- graph %>%
add_n_nodes(n = num_nodes, label = as.character(1:num_nodes))
# Add edges from the edge list
for (i in 1:nrow(edge_list)) {
graph <- graph %>%
add_edge(from = edge_list[i, 1], to = edge_list[i, 2])
}
# Render the graph (for large graphs, this might be slow)
render_graph(graph)
-33.02 +
-105.23 +
-66.95 +
-169.95 +
-17.63 +
-46.92 +
-80.00 +
-254.62 +
-455.12 +
-86.79 +
-96.30 +
-38.97 +
-208.12 +
-66.95 +
-66.95
# ----
# Set up code
rm( list = ls() )
setwd( "/Users/jyoung20/GitHub/TAWR2" )
text_v <- scan( "data/text/melville.txt", what = "character", sep = "\n" )
start_v <- which( text_v == "CHAPTER 1. Loomings." )
novel_lines_v <- text_v[start_v : length( text_v )]
chap_positions_v <- grep( "^CHAPTER \\d", novel_lines_v )
last_positions_v <- length( novel_lines_v )
chap_positions_v <- c( chap_positions_v, last_positions_v )
# this is the chapters
chapter_raws_l <- list()
#
chapter_freqs_l <- list()
for( i in 1:length( chap_positions_v ) ){
if( i != length( chap_positions_v ) ){
chapter_title <- novel_lines_v[chap_positions_v[i]]
start <- chap_positions_v[i] + 1
end <- chap_positions_v[i + 1] - 1
chapter_lines_v <- novel_lines_v[start:end]
chapter_words_v <- tolower( paste( chapter_lines_v, collapse = " " ) )
chapter_words_l <- strsplit( chapter_words_v, "\\W" )
chapter_word_v <- unlist( chapter_words_l )
chapter_word_v <- chapter_word_v[which( chapter_word_v != "" )]
chapter_freqs_t <- table( chapter_word_v )
chapter_raws_l[[chapter_title]] <- chapter_freqs_t
chapter_freqs_t_rel <- 100 * ( chapter_freqs_t / sum( chapter_freqs_t ) )
chapter_freqs_l[[chapter_title]] <- chapter_freqs_t_rel
}
}
# ----
# Chapter 7
# create an object with the mean number of words per chapter
mean_word_use_m <- do.call( rbind,
lapply( chapter_raws_l, mean )
)
# chapters with higher means use less lexical variety
# compared to chapters with lower means
# subtract the overall mean so the values
# represent deviation from the grand mean
mean_word_use_m_s <- scale( mean_word_use_m )
# create the type-token ratio
ttr_l <- lapply(
chapter_raws_l,
function( x ){ length( x ) / sum( x ) * 100 }
)
ttr_m <- do.call( rbind, ttr_l )
# number of words in each chapter
chapter_lengths_m <- do.call( rbind,
lapply( chapter_raws_l, sum )
)
# ----
# Chapter 8
# number of singletons, or hapaxes
chapter_hapax_v <- sapply( chapter_raws_l, function( x ) sum( x == 1 ) )
# percentage
hapax_percentage <- chapter_hapax_v / chapter_lengths_m
# ----
# Chapter 9
# set the path
input_dir <- "data/text"
# use a regular expression to get all the files that are .txt files
# in the directory; use the dir() function
files_v <- dir( input_dir, pattern = "\\.txt$", full.names = TRUE )
# write a function to better show the text data
show_files <- function( directory_path, pattern = "\\.txt$" ){
file_name_v <- dir( directory_path, pattern = "\\.txt$", full.names = TRUE )
for( i in seq_along( file_name_v ) ){
cat( i, file_name_v[i], "\n", sep = " " )
}
}
show_files( input_dir )
# function to return an ordered vector of words
make_token_v <- function( file_path, pattern = "\\W" ){
text_v <- scan( file_path, what = "character", sep = "\n" )
text_v <- paste( text_v, collapse = " " )
text_lower_v <- tolower( text_v )
text_words_v <- strsplit( text_lower_v, pattern )
text_words_v <- unlist( text_words_v )
text_words_v <- text_words_v[which( text_words_v != "" )]
return( text_words_v )
}
austen_word_v <- make_token_v( "data/text/austen.txt" )
# find position of the word "anguish"
positions_v <- which( austen_word_v == "anguish" )
# define first instance of "anguish"
first_instance <- positions_v[1]
# find words before and after it
austen_word_v[ ( first_instance - 1) : ( first_instance + 1 )]
# print it out
cat( austen_word_v[ ( first_instance - 1) : ( first_instance + 1 )] )
# now, second instance
cat( austen_word_v[ ( positions_v[2] - 1) : ( positions_v[2] + 1 )] )
# look for word dog
# print out a sentence with five words in front and five words behind
which( austen_word_v == "dog" )
cat( austen_word_v[ ( which( austen_word_v == "dog" ) - 5) : ( which( austen_word_v == "dog" ) + 5 )] )
# ----
# Chapter 9
cat( austen_word_v[ ( which( austen_word_v == "dog" ) - 5) : ( which( austen_word_v == "dog" ) + 5 )] )
getwed
getwd
getwd()
source( "code/corpus_functions.R")
rm( ls = ls() )
rm( ls = ls() )
rm( list = ls() )
source( "code/corpus_functions.R")
file.choose
file.choose()
myyear <- readline( "What year was Moby Dick published? \n" )
myyear
doitKwic <- function( directory_path ){
readline( show_files( directory_path ) )
}
doitKwic(input_dir)
input_dir <- "data/text"
# set output directory
output_dir <- "results"
doitKwic(input_dir)
# write function to execute KWIC more efficiently
doitKwic <- function( directory_path ){
file_id <- as.numeric( readline( show_files( directory_path ) ) )
keyword <- readline( "Enter a Keyword: " )
context <- as.numeric( readline( "How many words of context? " ) )
word_v <- make_token_v(
dir( directory_path, full.names = TRUE )[file_id]
)
hits_v <- which( word_v == keyword )
for( i in seq_along( hits_v ) ){
start <- hits_v[i] - context
end <- hits_v[i] + context
before <- word_v[ start: ( start + context - 1 ) ]
after <- word_v[ ( start + context + 1 ): end ]
keyword <- word_v[ start + context ]
cat( "---", i, "---", "\n" )
cat( before, "[", keyword, "]", after, "\n" )
}
}
doitKwic(input_dir)
doitKwic(input_dir)
# write function to execute KWIC more efficiently
doitKwic <- function( directory_path ){
file_id <- as.numeric( readline( show_files( directory_path ) ) )
keyword <- readline( "Enter a Keyword: " )
context <- as.numeric( readline( "How many words of context? " ) )
word_v <- make_token_v(
file.path( directory_path, dir( directory_path )[file_id] )
)
hits_v <- which( word_v == keyword )
for( i in seq_along( hits_v ) ){
start <- hits_v[i] - context
end <- hits_v[i] + context
before <- word_v[ start: ( start + context - 1 ) ]
after <- word_v[ ( start + context + 1 ): end ]
keyword <- word_v[ start + context ]
cat( "---", i, "---", "\n" )
cat( before, "[", keyword, "]", after, "\n" )
}
}
doitKwic(input_dir)
directory_path <- "data/text"
file_id <- as.numeric( readline( show_files( directory_path ) ) )
keyword <- readline( "Enter a Keyword: " )
context <- as.numeric( readline( "How many words of context? " ) )
word_v <- make_token_v(
file.path( directory_path, dir( directory_path )[file_id] )
)
make_token_v()
make_token_v
file.path( directory_path, dir( directory_path )[file_id] )
word_v <- make_token_v(
file.path( directory_path, dir( directory_path )[file_id] )
)
file_id
show_files( directory_path )
readline(show_files( directory_path ))
file_id <- as.numeric( readline( show_files( directory_path ) ) )
file_id <- as.numeric( readline( show_files( directory_path ) ) )
file_id
keyword <- readline( "Enter a Keyword: " )
word_v <- make_token_v(
file.path( directory_path, dir( directory_path )[file_id] )
)
# write function to execute KWIC more efficiently
doitKwic <- function( directory_path ){
file_id <- as.numeric( readline( show_files( directory_path ) ) )
keyword <- readline( "Enter a Keyword: " )
context <- as.numeric( readline( "How many words of context? " ) )
word_v <- make_token_v(
file.path( directory_path, dir( directory_path )[file_id] )
)
hits_v <- which( word_v == keyword )
for( i in seq_along( hits_v ) ){
start <- hits_v[i] - context
end <- hits_v[i] + context
before <- word_v[ start: ( start + context - 1 ) ]
after <- word_v[ ( start + context + 1 ): end ]
keyword <- word_v[ start + context ]
cat( "---", i, "---", "\n" )
cat( before, "[", keyword, "]", after, "\n" )
}
}
doitKwic(input_dir)
rm( list = ls() )
# call the functions you created in a separate script
source( "code/corpus_functions.R")
# set input directory
input_dir <- "data/text"
# set output directory
output_dir <- "results"
# using the readline function
myyear <- readline( "What year was Moby Dick published? \n" )
# try the function
doitKwic( input_dir )
