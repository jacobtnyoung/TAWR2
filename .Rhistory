# ----
# Set up code
rm( list = ls() )
setwd( "/Users/jyoung20/GitHub/TAWR2" )
text_v <- scan( "data/text/melville.txt", what = "character", sep = "\n" )
start_v <- which( text_v == "CHAPTER 1. Loomings." )
novel_lines_v <- text_v[start_v : length( text_v )]
chap_positions_v <- grep( "^CHAPTER \\d", novel_lines_v )
last_positions_v <- length( novel_lines_v )
chap_positions_v <- c( chap_positions_v, last_positions_v )
# this is the chapters
chapter_raws_l <- list()
#
chapter_freqs_l <- list()
for( i in 1:length( chap_positions_v ) ){
if( i != length( chap_positions_v ) ){
chapter_title <- novel_lines_v[chap_positions_v[i]]
start <- chap_positions_v[i] + 1
end <- chap_positions_v[i + 1] - 1
chapter_lines_v <- novel_lines_v[start:end]
chapter_words_v <- tolower( paste( chapter_lines_v, collapse = " " ) )
chapter_words_l <- strsplit( chapter_words_v, "\\W" )
chapter_word_v <- unlist( chapter_words_l )
chapter_word_v <- chapter_word_v[which( chapter_word_v != "" )]
chapter_freqs_t <- table( chapter_word_v )
chapter_raws_l[[chapter_title]] <- chapter_freqs_t
chapter_freqs_t_rel <- 100 * ( chapter_freqs_t / sum( chapter_freqs_t ) )
chapter_freqs_l[[chapter_title]] <- chapter_freqs_t_rel
}
}
# ----
#
chapter_freqs_l
class(chapter_freqs_l)
chapter_freqs_l[[1]]
sort(chapter_freqs_l[[1]]
)
sort(chapter_freqs_l[[1]], ascending = FALSE)
chapter_raws_l[[1]]
class(chapter_raws_l[[1]])
str(chapter_raws_l)
mean_word_use_m <- do.call( rbind,
lapply( chapter_raws_l, mean )
)
mean_word_use_m
mean_word_use_m_s <- scale( mean_word_use_m )
mean_word_use_m_s
sort(mean_word_use_m_s)
which(mean_word_use_m_s == max( mean_word_use_m_s))
which(mean_word_use_m_s == min( mean_word_use_m_s))
mean_word_use_m[122]
order( mean_word_use_m )
order( mean_word_use_m, decreasing = TRUE )
mean_word_use_m[order( mean_word_use_m, decreasing = TRUE ),]
ttr_l <- lapply(
chapter_raws_l,
function( x ){ length( x ) / sum( x ) * 100 }
)
ttr_l
ttr_m <- do.call( rbind, ttr_l )
ttr_m
ttr_n <- lapply( chapter_raws_l, sum )
ttrn_n
ttr_n
cor( as.vector( ttr_m ), as.vector( ttr_n ) )
cor( as.vector( ttr_m ), as.vector( ttr_n ) )
as.vector( ttr_m )
as.vector( ttr_n )
cor( as.vector( ttr_m ), ttr_n )
ttr_n <- lapply( chapter_raws_l, sum, 1 )
ttr_n
ttr_n <- lapply( chapter_raws_l, sum, 2 )
ttr_n
chapter_words <- do.call( rbind,
lapply( chapter_raws_l, sum )
)
chapter_words
cor( as.vector( ttr_m ), chapter_words )
cor( mean_word_use_m, chapter_words )
cor( mean_word_use_m, chapter_words )
chapter_hapax_v <- sapply( chapter_raws_l, function( x ) sum( x == 1 ) )
chapter_hapax_v
sort(chapter_hapax_v)
chapter_raws_l[[1]]
sum( chapter_raws_l[[1]] == 1 )
chapter_hapax_v
hapax_percentage <- chapter_hapax_v / chapter_lengths_m
chapter_lengths_m <- do.call( rbind,
lapply( chapter_raws_l, sum )
)
cor( as.vector( ttr_m ), chapter_lengths_m )
# 2
cor( as.vector( ttr_m ), chapter_lengths_m )
# ----
# Chapter 8
# number of singletons, or hapaxes
chapter_hapax_v <- sapply( chapter_raws_l, function( x ) sum( x == 1 ) )
# percentage
hapax_percentage <- chapter_hapax_v / chapter_lengths_m
hapax_percentage
input_dir <- "data/text"
# use a regular expression to get all the files that are .txt files
# in the directory
files_v <- dir( input_dir, "\\.txt$" )
files_v
files_v <- dir( input_dir, "\\.txt$", full.names = TRUE )
files_v
?dir
list.files( input_dir, "\\.txt$", full.names = TRUE )
list.files( input_dir, "\\.txt$" )
show_files <- function( directory_path, pattern = "\\.txt$" ){
files_names_v <- dir( directory_path, pattern = "\\.txt$", full.names = TRUE )
for( i in seq_along( file_names_v ) ){
cat( i, files_names_v[i], "\n", sep = " " )
}
}
show_files( input_dir )
show_files <- function( directory_path, pattern = "\\.txt$" ){
file_name_v <- dir( directory_path, pattern = "\\.txt$", full.names = TRUE )
for( i in seq_along( file_name_v ) ){
cat( i, file_name_v[i], "\n", sep = " " )
}
}
show_files( input_dir )
make_token_v <- function( file_path, pattern ){
text_v <- scan( file_path, what = "character", sep = "\n" )
text_v <- paste( text_v, collapse = " " )
text_lower_v <- tolower( text_v )
text_words_v <- strsplit( text_lower_v, pattern )
text_words_v <- unlist( text_words_v )
text_words_v <- text_words_v[which( text_words_v != "" )]
return( text_words_v )
}
austen_word_v <- make_token_v( "data/text/austen.txt" )
make_token_v <- function( file_path, pattern = "\\W" ){
text_v <- scan( file_path, what = "character", sep = "\n" )
text_v <- paste( text_v, collapse = " " )
text_lower_v <- tolower( text_v )
text_words_v <- strsplit( text_lower_v, pattern )
text_words_v <- unlist( text_words_v )
text_words_v <- text_words_v[which( text_words_v != "" )]
return( text_words_v )
}
austen_word_v <- make_token_v( "data/text/austen.txt" )
positions_v <- which( austen_word_v == "anguish" )
positions_v
first_instance <- positions_v[1]
positions_v[1]-1
austen_word_v[first_instance]
austen_word_v[ first_instance-1:first_instance+1]
first_instance-1
first_instance+1
austen_word_v[ ( first_instance - 1) : ( first_instance + 1 )]
cat(austen_word_v[ ( first_instance - 1) : ( first_instance + 1 )])
cat( austen_word_v[ ( first_instance - 1) : ( first_instance + 1 )] )
cat( austen_word_v[ ( positions_v[2] - 1) : ( positions_v[2] + 1 )] )
cat( austen_word_v[ ( which( austen_word_v == "dog" ) - 5) : ( which( austen_word_v == "dog" ) + 5 )] )
which( austen_word_v == "dog" )
